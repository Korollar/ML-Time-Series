{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  This notebook shows an example of neural networks applied to time series classification using Tensorflow. We build a 2-layer neural network with softmax activation and use the raw time series as input for our neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "import feature_extraction_functions as fef\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn import preprocessing\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set we use is x-axis acceleration of people performing 6 different activities (walking (1), walking upstairs (2), walking downstairs (3), sitting (4), standing (5) laying (6)). More details about the data set [here](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape : (7352, 128)\n",
      "X_test shape : (2947, 128)\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../data')\n",
    "# Raw signals\n",
    "# X axis\n",
    "X_train = np.loadtxt('X_train.txt')\n",
    "X_test = np.loadtxt('X_test.txt')\n",
    "\n",
    "print(\"X_train shape : {}\".format(X_train.shape))\n",
    "print(\"X_test shape : {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape : (7352, 6)\n",
      "y_test shape : (2947, 6)\n"
     ]
    }
   ],
   "source": [
    "# Label vectors\n",
    "y_train = np.loadtxt('y_train.txt') - 1\n",
    "y_train = (np.arange(np.unique(y_train).shape[0]) == y_train[:, None]).astype(int)\n",
    "\n",
    "y_test = np.loadtxt('y_test.txt') - 1\n",
    "y_test = (np.arange(np.unique(y_test).shape[0]) == y_test[:, None]).astype(int)\n",
    "\n",
    "print(\"y_train shape : {}\".format(y_train.shape))\n",
    "print(\"y_test shape : {}\".format(y_test.shape))\n",
    "\n",
    "label_names = ['Walking', 'Walking upstairs', 'Walking downstairs', 'Sitting', 'Standing', 'Laying']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Storing number of features and number of labels for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = X_train.shape[1]\n",
    "n_labels = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "training_epoch = 1500\n",
    "batch_size = 100\n",
    "display_step = 100\n",
    "num_layer_1 = n_features\n",
    "num_layer_2 = n_features #(n_features + n_labels) // 2\n",
    "l = 0.0\n",
    "prob = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Tensorflow graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Neural Network input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, n_features])\n",
    "y = tf.placeholder(tf.float32, shape=[None, n_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here we build the actual network with two layers: the first layer a n_features number of neurons with softmax activation and a second layer with num_layer_2 neurons and no activation function since we are going to apply softmax to the output of this second layer when defining the neural network cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1st hidden layer\n",
    "W_1 = tf.Variable(tf.random_normal([num_layer_1, num_layer_2], stddev=0.01))\n",
    "b_1 = tf.Variable(tf.zeros([num_layer_2]))\n",
    "y_1 = tf.nn.softmax(tf.matmul(x, W_1) + b_1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "y_pred = tf.nn.dropout(y_pred, keep_prob)\n",
    "\n",
    "# Output layer\n",
    "W_2 = tf.Variable(tf.random_normal([num_layer_2, n_labels], stddev=0.01))\n",
    "b_2 = tf.Variable(tf.zeros([n_labels]))\n",
    "y_pred = tf.matmul(y_1, W_2) + b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Cost and optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lam = tf.placeholder(tf.float32)\n",
    "\n",
    "# Cost function with L2 regularization\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_pred) + \n",
    "                      lam * (tf.nn.l2_loss(W_1) + tf.nn.l2_loss(W_2)))\n",
    "# optimizer\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "# prediction\n",
    "prediction = tf.nn.softmax(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramters:\n",
      "learning rate = 0.05\n",
      "training_epoch = 1500\n",
      "batch_size = 100\n",
      "-- Network structure -- \n",
      "num_layer_1 = 128\n",
      "num_layer_2 = 128\n",
      "-- Regularization --\n",
      "drop out prob = 1.0\n",
      "L2 regularization = 0.0\n",
      "\n",
      "\n",
      "Epoch n°    0. cost = 1.30194486.\n",
      "Epoch n°  100. cost = 0.65251704.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "cost_log = []\n",
    "\n",
    "print(\"Paramters:\")\n",
    "print(\"learning rate = {}\".format(learning_rate))\n",
    "print(\"training_epoch = {}\".format(training_epoch))\n",
    "print(\"batch_size = {}\".format(batch_size))\n",
    "print(\"-- Network structure -- \")\n",
    "print(\"num_layer_1 = {}\".format(num_layer_1))\n",
    "print(\"num_layer_2 = {}\".format(num_layer_2))\n",
    "print(\"-- Regularization --\")\n",
    "print(\"drop out prob = {}\".format(prob))\n",
    "print(\"L2 regularization = {}\".format(l))\n",
    "print(\"\\n\")\n",
    "\n",
    "for epoch in range(training_epoch):\n",
    "    avg_cost = 0.\n",
    "    num_batch = X_train.shape[0] // batch_size\n",
    "    \n",
    "    # We first shuffle the training data\n",
    "    shuffle = np.random.permutation(X_train.shape[0])\n",
    "    X_train_s = X_train[shuffle, :]\n",
    "    y_train_s = y_train[shuffle, :]\n",
    "    \n",
    "    for i in range(num_batch):\n",
    "        batch_x = X_train_s[int(i*batch_size):int((i+1)*batch_size), :]\n",
    "        batch_y = y_train_s[int(i*batch_size):int((i+1)*batch_size), :]\n",
    "        train = sess.run(train_step, feed_dict={x: batch_x, y: batch_y, keep_prob: prob, lam: l})\n",
    "        c =  sess.run(cost, feed_dict={x: batch_x, y: batch_y, keep_prob: 1., lam: 0.})        \n",
    "        avg_cost += c/num_batch\n",
    "        \n",
    "    # Train and cv cost log\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch n° {:4d}. cost = {:.8f}.\".format(epoch, avg_cost))\n",
    "        \n",
    "    cost_log.append(avg_cost)\n",
    "    \n",
    "\n",
    "print(\"Training over...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(range(len(cost_log)), cost_log)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.title(\"Training set cost\")\n",
    "\n",
    "\n",
    "y_train_pred = sess.run(tf.argmax(prediction,1), feed_dict={x: X_train, keep_prob: 1.})\n",
    "print('Training set report')\n",
    "print(classification_report(np.argmax(y_train, axis=1), y_train_pred, target_names=label_names))\n",
    "plt.figure(3)\n",
    "fef.plot_confusion_matrix(confusion_matrix(np.argmax(y_train, axis=1), y_train_pred), label_names)\n",
    "\n",
    "y_test_pred = sess.run(tf.argmax(prediction,1), feed_dict={x: X_test, keep_prob: 1.})\n",
    "print('Test set report')\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_test_pred, target_names=label_names))\n",
    "plt.figure(4)\n",
    "fef.plot_confusion_matrix(confusion_matrix(np.argmax(y_test, axis=1), y_test_pred), label_names)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Paramters:\n",
    "learning rate = 0.05\n",
    "training_epoch = 6000\n",
    "batch_size = 100\n",
    "-- Network structure -- \n",
    "num_layer_1 = 128\n",
    "num_layer_2 = 128\n",
    "-- Regularization --\n",
    "drop out prob = 1.0\n",
    "L2 regularization = 0.0\n",
    "\n",
    "\n",
    "Epoch n°    0. cost = 1.29358814.\n",
    "Epoch n°  100. cost = 0.62734286.\n",
    "Epoch n°  200. cost = 0.59615321.\n",
    "Epoch n°  300. cost = 0.55617541.\n",
    "Epoch n°  400. cost = 0.55240083.\n",
    "Epoch n°  500. cost = 0.51640089.\n",
    "Epoch n°  600. cost = 0.52212010.\n",
    "Epoch n°  700. cost = 0.47001223.\n",
    "Epoch n°  800. cost = 0.45092447.\n",
    "Epoch n°  900. cost = 0.43522266.\n",
    "Epoch n° 1000. cost = 0.41911411.\n",
    "Epoch n° 1100. cost = 0.45977154.\n",
    "Epoch n° 1200. cost = 0.37950618.\n",
    "Epoch n° 1300. cost = 0.37941976.\n",
    "Epoch n° 1400. cost = 0.36402601.\n",
    "Epoch n° 1500. cost = 0.36181774.\n",
    "Epoch n° 1600. cost = 0.35305573.\n",
    "Epoch n° 1700. cost = 0.32691913.\n",
    "Epoch n° 1800. cost = 0.33533966.\n",
    "Epoch n° 1900. cost = 0.32368060.\n",
    "Epoch n° 2000. cost = 0.30189006.\n",
    "Epoch n° 2100. cost = 0.29234168.\n",
    "Epoch n° 2200. cost = 0.32200553.\n",
    "Epoch n° 2300. cost = 0.28747340.\n",
    "Epoch n° 2400. cost = 0.29338322.\n",
    "Epoch n° 2500. cost = 0.29864604.\n",
    "Epoch n° 2600. cost = 0.29692236.\n",
    "Epoch n° 2700. cost = 0.28090668.\n",
    "Epoch n° 2800. cost = 0.26183263.\n",
    "Epoch n° 2900. cost = 0.25052612.\n",
    "Epoch n° 3000. cost = 0.25763807.\n",
    "Epoch n° 3100. cost = 0.24576370.\n",
    "Epoch n° 3200. cost = 0.28468481.\n",
    "Epoch n° 3300. cost = 0.23438251.\n",
    "Epoch n° 3400. cost = 0.23968151.\n",
    "Epoch n° 3500. cost = 0.24386988.\n",
    "Epoch n° 3600. cost = 0.28600749.\n",
    "Epoch n° 3700. cost = 0.22991039.\n",
    "Epoch n° 3800. cost = 0.21870220.\n",
    "Epoch n° 3900. cost = 0.23165407.\n",
    "Epoch n° 4000. cost = 0.22822775.\n",
    "Epoch n° 4100. cost = 0.21830588.\n",
    "Epoch n° 4200. cost = 0.21708936.\n",
    "Epoch n° 4300. cost = 0.22156327.\n",
    "Epoch n° 4400. cost = 0.21511520.\n",
    "Epoch n° 4500. cost = 0.19481707.\n",
    "Epoch n° 4600. cost = 0.20703194.\n",
    "Epoch n° 4700. cost = 0.22103408.\n",
    "Epoch n° 4800. cost = 0.19173198.\n",
    "Epoch n° 4900. cost = 0.20647653.\n",
    "Epoch n° 5000. cost = 0.18102213.\n",
    "Epoch n° 5100. cost = 0.21170738.\n",
    "Epoch n° 5200. cost = 0.18653505.\n",
    "Epoch n° 5300. cost = 0.17519726.\n",
    "Epoch n° 5400. cost = 0.18856200.\n",
    "Epoch n° 5500. cost = 0.17052807.\n",
    "Epoch n° 5600. cost = 0.17467913.\n",
    "Epoch n° 5700. cost = 0.17803476.\n",
    "Epoch n° 5800. cost = 0.20223067.\n",
    "Epoch n° 5900. cost = 0.16515660.\n",
    "Training over...\n",
    "\n",
    "\n",
    "Training set report\n",
    "                    precision    recall  f1-score   support\n",
    "\n",
    "           Walking       0.98      0.94      0.96      1226\n",
    "  Walking upstairs       0.87      0.97      0.92      1073\n",
    "Walking downstairs       0.98      0.91      0.94       986\n",
    "           Sitting       0.94      0.90      0.92      1286\n",
    "          Standing       0.94      0.94      0.94      1374\n",
    "            Laying       0.92      0.95      0.94      1407\n",
    "\n",
    "       avg / total       0.94      0.94      0.94      7352\n",
    "\n",
    "Test set report\n",
    "                    precision    recall  f1-score   support\n",
    "\n",
    "           Walking       0.72      0.69      0.71       496\n",
    "  Walking upstairs       0.56      0.69      0.62       471\n",
    "Walking downstairs       0.73      0.60      0.66       420\n",
    "           Sitting       0.37      0.36      0.36       491\n",
    "          Standing       0.45      0.43      0.44       532\n",
    "            Laying       0.54      0.56      0.55       537\n",
    "\n",
    "       avg / total       0.56      0.55      0.55      2947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-89-1ae738681042>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-89-1ae738681042>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Paramters:\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Paramters:\n",
    "learning rate = 0.01\n",
    "training_epoch = 1500\n",
    "batch_size = 100\n",
    "-- Network structure -- \n",
    "num_layer_1 = 128\n",
    "num_layer_2 = 128\n",
    "-- Regularization --\n",
    "drop out prob = 0.5\n",
    "L2 regularization = 0.0\n",
    "\n",
    "\n",
    "Epoch n°    0. cost = 1.69495740.\n",
    "Epoch n°  100. cost = 0.72698880.\n",
    "Epoch n°  200. cost = 0.65853535.\n",
    "Epoch n°  300. cost = 0.62282811.\n",
    "Epoch n°  400. cost = 0.60026161.\n",
    "Epoch n°  500. cost = 0.58805539.\n",
    "Epoch n°  600. cost = 0.57424411.\n",
    "Epoch n°  700. cost = 0.56162119.\n",
    "Epoch n°  800. cost = 0.55529996.\n",
    "Epoch n°  900. cost = 0.54978269.\n",
    "Epoch n° 1000. cost = 0.54558178.\n",
    "Epoch n° 1100. cost = 0.53990666.\n",
    "Epoch n° 1200. cost = 0.54018043.\n",
    "Epoch n° 1300. cost = 0.53091570.\n",
    "Epoch n° 1400. cost = 0.52653392.\n",
    "Training over...\n",
    "\n",
    "\n",
    "Training set report\n",
    "                    precision    recall  f1-score   support\n",
    "\n",
    "           Walking       1.00      1.00      1.00      1226\n",
    "  Walking upstairs       1.00      0.97      0.99      1073\n",
    "Walking downstairs       0.97      1.00      0.99       986\n",
    "           Sitting       0.42      0.24      0.30      1286\n",
    "          Standing       0.48      0.79      0.60      1374\n",
    "            Laying       0.73      0.56      0.63      1407\n",
    "\n",
    "       avg / total       0.75      0.74      0.73      7352\n",
    "\n",
    "Test set report\n",
    "                    precision    recall  f1-score   support\n",
    "\n",
    "           Walking       0.71      0.88      0.79       496\n",
    "  Walking upstairs       0.69      0.55      0.61       471\n",
    "Walking downstairs       0.77      0.65      0.70       420\n",
    "           Sitting       0.33      0.19      0.24       491\n",
    "          Standing       0.43      0.69      0.53       532\n",
    "            Laying       0.70      0.60      0.65       537\n",
    "\n",
    "       avg / total       0.60      0.59      0.58      2947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Paramters:\n",
    "learning rate = 0.1\n",
    "training_epoch = 1500\n",
    "batch_size = 100\n",
    "num_layer_1 = 128\n",
    "num_layer_2 = 67\n",
    "drop out prob = 1.0\n",
    "L2 regularization = 0.01\n",
    "\n",
    "\n",
    "Epoch n°    0. cost = 1.20719103.\n",
    "//anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
    "  'precision', 'predicted', average, warn_for)\n",
    "f1 train = 0.2789\n",
    "f1 test = 0.3002\n",
    "Epoch n°  100. cost = 0.75465302.\n",
    "f1 train = 0.6461\n",
    "f1 test = 0.5750\n",
    "Epoch n°  200. cost = 0.66458246.\n",
    "f1 train = 0.6810\n",
    "f1 test = 0.5738\n",
    "Epoch n°  300. cost = 0.59180169.\n",
    "f1 train = 0.7346\n",
    "f1 test = 0.5902\n",
    "Epoch n°  400. cost = 0.51990438.\n",
    "f1 train = 0.7430\n",
    "f1 test = 0.5763\n",
    "Epoch n°  500. cost = 0.47497643.\n",
    "f1 train = 0.7798\n",
    "f1 test = 0.5744\n",
    "Epoch n°  600. cost = 0.43417974.\n",
    "f1 train = 0.7902\n",
    "f1 test = 0.5654\n",
    "Epoch n°  700. cost = 0.41675655.\n",
    "f1 train = 0.8056\n",
    "f1 test = 0.5692\n",
    "Epoch n°  800. cost = 0.37648067.\n",
    "f1 train = 0.8355\n",
    "f1 test = 0.5560\n",
    "Epoch n°  900. cost = 0.33151524.\n",
    "f1 train = 0.8514\n",
    "f1 test = 0.5654\n",
    "Epoch n° 1000. cost = 0.29555754.\n",
    "f1 train = 0.8626\n",
    "f1 test = 0.5596\n",
    "Epoch n° 1100. cost = 0.27434672.\n",
    "f1 train = 0.8791\n",
    "f1 test = 0.5518\n",
    "Epoch n° 1200. cost = 0.25071624.\n",
    "f1 train = 0.8818\n",
    "f1 test = 0.5670\n",
    "Epoch n° 1300. cost = 0.25381679.\n",
    "f1 train = 0.8900\n",
    "f1 test = 0.5621\n",
    "Epoch n° 1400. cost = 0.27323280.\n",
    "f1 train = 0.8681\n",
    "f1 test = 0.5586\n",
    "Training over...\n",
    "\n",
    "\n",
    "Training set report\n",
    "                    precision    recall  f1-score   support\n",
    "\n",
    "           Walking       0.87      0.96      0.91      1226\n",
    "  Walking upstairs       0.73      0.83      0.78      1073\n",
    "Walking downstairs       0.96      0.71      0.82       986\n",
    "           Sitting       0.98      0.83      0.89      1286\n",
    "          Standing       0.92      0.95      0.94      1374\n",
    "            Laying       0.89      0.99      0.94      1407\n",
    "\n",
    "       avg / total       0.90      0.89      0.89      7352\n",
    "\n",
    "Test set report\n",
    "                    precision    recall  f1-score   support\n",
    "\n",
    "           Walking       0.69      0.82      0.75       496\n",
    "  Walking upstairs       0.60      0.69      0.64       471\n",
    "Walking downstairs       0.77      0.51      0.61       420\n",
    "           Sitting       0.38      0.32      0.35       491\n",
    "          Standing       0.42      0.40      0.41       532\n",
    "            Laying       0.51      0.59      0.55       537\n",
    "\n",
    "       avg / total       0.55      0.55      0.55      2947\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Paramters:\n",
    "learning rate = 0.1\n",
    "training_epoch = 500\n",
    "batch_size = 100\n",
    "num_layer_1 = 128\n",
    "num_layer_2 = 128\n",
    "drop out prob = 1.0\n",
    "L2 regularization = 0.01\n",
    "\n",
    "\n",
    "Epoch n°    0. cost = 1.21072758.\n",
    "cost_t = 0.00014542. cost_cv = 0.00036468.\n",
    "Epoch n°   50. cost = 0.81807568.\n",
    "cost_t = 0.00011210. cost_cv = 0.00034152.\n",
    "Epoch n°  100. cost = 0.72975950.\n",
    "cost_t = 0.00009868. cost_cv = 0.00040344.\n",
    "Epoch n°  150. cost = 0.65608785.\n",
    "cost_t = 0.00009032. cost_cv = 0.00046475.\n",
    "Epoch n°  200. cost = 0.58742912.\n",
    "cost_t = 0.00008191. cost_cv = 0.00058560.\n",
    "Epoch n°  250. cost = 0.49851472.\n",
    "cost_t = 0.00006760. cost_cv = 0.00077760.\n",
    "Epoch n°  300. cost = 0.45754595.\n",
    "cost_t = 0.00006477. cost_cv = 0.00092820.\n",
    "Epoch n°  350. cost = 0.42982763.\n",
    "cost_t = 0.00005749. cost_cv = 0.00116620.\n",
    "Epoch n°  400. cost = 0.40677153.\n",
    "cost_t = 0.00005422. cost_cv = 0.00124392.\n",
    "Epoch n°  450. cost = 0.36661577.\n",
    "cost_t = 0.00005177. cost_cv = 0.00148626.\n",
    "Training over...\n",
    "Training set report\n",
    "                    precision    recall  f1-score   support\n",
    "\n",
    "           Walking       0.85      0.88      0.86      1226\n",
    "  Walking upstairs       0.81      0.79      0.80      1073\n",
    "Walking downstairs       0.82      0.83      0.83       986\n",
    "           Sitting       0.95      0.68      0.79      1286\n",
    "          Standing       0.75      0.98      0.85      1374\n",
    "            Laying       0.94      0.90      0.92      1407\n",
    "\n",
    "       avg / total       0.86      0.85      0.85      7352\n",
    "\n",
    "Test set report\n",
    "                    precision    recall  f1-score   support\n",
    "\n",
    "           Walking       0.68      0.72      0.70       496\n",
    "  Walking upstairs       0.62      0.62      0.62       471\n",
    "Walking downstairs       0.63      0.57      0.59       420\n",
    "           Sitting       0.32      0.26      0.29       491\n",
    "          Standing       0.43      0.56      0.49       532\n",
    "            Laying       0.58      0.53      0.55       537\n",
    "\n",
    "       avg / total       0.54      0.54      0.54      2947"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
