{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show how to build a Convolutional Neural Network (CNN) with TensorFlow to classify time series. The task here is to classify 6 different types of activities (walking, walking upstairs, walking downstairs, sitting, standing, laying) based on x, y and z accelerometer signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We first load the raw accelerometer signals (x, y and z axis). Each sample from the data set is a 2.56s window of an activity being performed recorded at a 50Hz rate which make 128 readings per sample, per axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_x_raw shape : (7352, 128)\n",
      "X_test_x_raw shape : (2947, 128)\n",
      "X_train_y_raw shape : (7352, 128)\n",
      "X_test_y_raw shape : (2947, 128)\n",
      "X_train_z_raw shape : (7352, 128)\n",
      "X_test_z_raw shape : (2947, 128)\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../data')\n",
    "# Raw signals\n",
    "# X axis\n",
    "X_train_x_raw = np.loadtxt('X_train.txt')\n",
    "X_test_x_raw = np.loadtxt('X_test.txt')\n",
    "# Y axis\n",
    "X_train_y_raw = np.loadtxt('Xy_train.txt')\n",
    "X_test_y_raw = np.loadtxt('Xy_test.txt')\n",
    "# Z axis\n",
    "X_train_z_raw = np.loadtxt('Xz_train.txt')\n",
    "X_test_z_raw = np.loadtxt('Xz_test.txt')\n",
    "\n",
    "print(\"X_train_x_raw shape : {}\".format(X_train_x_raw.shape))\n",
    "print(\"X_test_x_raw shape : {}\".format(X_test_x_raw.shape))\n",
    "print(\"X_train_y_raw shape : {}\".format(X_train_y_raw.shape))\n",
    "print(\"X_test_y_raw shape : {}\".format(X_test_y_raw.shape))\n",
    "print(\"X_train_z_raw shape : {}\".format(X_train_z_raw.shape))\n",
    "print(\"X_test_z_raw shape : {}\".format(X_test_z_raw.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the signals (standard score normalization i.e. for each signal we substract its mean and divide it by its standard deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standard_score_normalization(x):\n",
    "    x_m = np.mean(x, axis=1).reshape(-1,1)\n",
    "    x_std = np.std(x, axis=1).reshape(-1,1)\n",
    "    return (x - x_m)/x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scaling raw signals: Standard Score \n",
    "X_train_x_raw = standard_score_normalization(X_train_x_raw)\n",
    "X_test_x_raw = standard_score_normalization(X_test_x_raw)\n",
    "\n",
    "X_train_y_raw = standard_score_normalization(X_train_y_raw)\n",
    "X_test_y_raw = standard_score_normalization(X_test_y_raw)\n",
    "\n",
    "X_train_z_raw = standard_score_normalization(X_train_z_raw)\n",
    "X_test_z_raw = standard_score_normalization(X_test_z_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Now we are going to build/reshape our training and test sets. We want each sample of the training/test set to have the following shape (number of samples, time serie height, time serie length, number of channels) with:\n",
    "  - number of samples: the number of samples in the training or test set.\n",
    "  - time serie height: the \"height\" just like the height of an image except that as we are considering 1D data, our \"height\" will be set to 1. \n",
    "  - time serie length: this corresponds to the number of readings in the time series, in our case 128.\n",
    "  - number of channels: in image classification taks, this dimension would represent the 3 RGB levels of a pixel but in our case this will be the 3 dimensions of the accelerometer data: x, y and z.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below reshapes the 3 raw signal arrays as we have explained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def concatenate_xyz(x,y,z):\n",
    "    data = np.empty((x.shape[0],1,x.shape[1],3))\n",
    "    for i, rowx, rowy, rowz in zip(range(x.shape[0]),x,y,z):\n",
    "        data[i,:,:,0] = rowx.reshape(1,-1)\n",
    "        data[i,:,:,1] = rowy.reshape(1,-1)\n",
    "        data[i,:,:,2] = rowz.reshape(1,-1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 1, 128, 3)\n",
      "(2947, 1, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train = concatenate_xyz(X_train_x_raw, X_train_y_raw, X_train_z_raw)\n",
    "X_test = concatenate_xyz(X_test_x_raw, X_test_y_raw, X_test_z_raw)\n",
    "\n",
    "print(\"X_train shape : {}\".format(X_train.shape))\n",
    "print(\"X_test shape : {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training and test sets labels and we convert them to 1 hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 6)\n",
      "(2947, 6)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.loadtxt('y_train.txt') - 1\n",
    "y_train = (np.arange(np.unique(y_train).shape[0]) == y_train[:, None]).astype(int)\n",
    "\n",
    "y_test = np.loadtxt('y_test.txt') - 1\n",
    "y_test = (np.arange(np.unique(y_test).shape[0]) == y_test[:, None]).astype(int)\n",
    "\n",
    "print(\"y_train shape : {}\".format(y_train.shape))\n",
    "print(\"y_test shape : {}\".format(y_test.shape))\n",
    "\n",
    "label_names = ['Walking', 'Walking upstairs', 'Walking downstairs', 'Sitting', 'Standing', 'Laying']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for label in range(6):\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize = (10, 7), sharex = True)\n",
    "    ax0.plot(range(X_train_x_raw.shape[1]), X_train_x_raw[(np.argmax(y_train,1) == label),:][200])\n",
    "    ax0.set_title('x axis')\n",
    "    ax1.plot(range(X_train_x_raw.shape[1]), X_train_y_raw[(np.argmax(y_train,1) == label),:][200])\n",
    "    ax1.set_title('y axis')\n",
    "    ax2.plot(range(X_train_x_raw.shape[1]), X_train_z_raw[(np.argmax(y_train,1) == label),:][200])\n",
    "    ax2.set_title('z axis')\n",
    "    ax0.set_xlim([0,X_train_x_raw.shape[1]-1])\n",
    "    fig.suptitle(label_names[label])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below help us build the CNN :\n",
    "  - We easily define weights and biases variables with the two first functions.\n",
    "  - The third function helps us build convolutional layers with pooling and a relu activation.  \n",
    "  - Finally, the last function builds fully connected layers with or without relu activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape=shape, stddev=0.01))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.zeros(shape=shape))\n",
    "\n",
    "def convolutional_layer(input,\n",
    "                        input_channels, # number of input channels for the layer\n",
    "                        filter_height, # will be set to 1 as we deal with 1D data\n",
    "                        filter_width, # width of the filter\n",
    "                        output_channels, # number of ouput channels for the layer\n",
    "                        conv_stride=1, # convolution stride\n",
    "                        pooling_size=1, # size of the pooling\n",
    "                        pooling_stride=1): # stride of the pooling\n",
    "    \n",
    "    filter_weights = weight_variable([filter_height, filter_width, input_channels, output_channels])\n",
    "    filter_biases = bias_variable([output_channels])\n",
    "    # applying convolution to input\n",
    "    conv_layer = tf.nn.conv2d(input, \n",
    "                              filter=filter_weights, \n",
    "                              strides=[1,1,conv_stride,1], \n",
    "                              padding = 'SAME')\n",
    "    conv_layer += filter_biases\n",
    "    # applying pooling to convolution layer\n",
    "    conv_layer = tf.nn.max_pool(conv_layer,\n",
    "                                ksize=[1,1,pooling_size,1], \n",
    "                                strides=[1,1,pooling_stride,1],\n",
    "                                padding='SAME')\n",
    "    # applying relu \n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    return conv_layer\n",
    "\n",
    "def fully_connected_layer(input, num_input, num_output, relu=True):\n",
    "    \n",
    "    W = weight_variable(shape=[num_input, num_output])\n",
    "    b = bias_variable(shape=[num_output])\n",
    "    if relu:\n",
    "        logit = tf.nn.relu(tf.matmul(input,W) + b)\n",
    "    else:\n",
    "        logit = tf.matmul(input,W) + b\n",
    "        \n",
    "    return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network and training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We are going to build a CNN with two convolutional layers whose output will be fed into a 2 connected layer neural network. We first define the input shape as we explained previously (shape = (number of samples, time serie height, time serie length, number of channels), then we define the two convolutional layers structure (number of input channels, number of output channels, filter width... note that we do not define filter height as it is set to 1), the we define the number neurons in the hidden layer and finally we define the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input parameters\n",
    "filter_height = X_train.shape[1] # = 1 as we deal with 1D data\n",
    "ts_length = X_train.shape[2] # = 128 as mentionned before\n",
    "input_channels = X_train.shape[3] # = 3 since we consider the 3 axis of the accelerometer data\n",
    "num_labels = y_train.shape[1] # = 6, 6 different activities...\n",
    "\n",
    "# 1st convolutional layer parameters\n",
    "input_channels_1 = input_channels \n",
    "filter_width_1 = 30\n",
    "output_channels_1 = 14\n",
    "\n",
    "# 2nd convolutional layer parameters\n",
    "input_channels_2 = output_channels_1\n",
    "filter_width_2 = 15\n",
    "output_channels_2 = 36\n",
    "\n",
    "# Hiden layer parameters\n",
    "num_hidden = 1000\n",
    "\n",
    "# Training paramters\n",
    "learning_rate = 1e-4\n",
    "training_epoch = 100\n",
    "batch_size = 10\n",
    "display_step = 10\n",
    "prob = 0.5 # dropout probability to keep a neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our graph inputs with the required shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, filter_height, ts_length, input_channels])\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the two convolutional layers. The convolutional stride is set to 1 for both of them. We set the pooling size to 10 and the corresponding stride to 2 which means we will take the max of 10 readings windows and slide the window of 2 readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_layer_1 = convolutional_layer(x, \n",
    "                                   input_channels_1, \n",
    "                                   filter_height,\n",
    "                                   filter_width_1,\n",
    "                                   output_channels_1,\n",
    "                                   conv_stride=1,\n",
    "                                   pooling_size=10,\n",
    "                                   pooling_stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_layer_2 = convolutional_layer(conv_layer_1, \n",
    "                                   input_channels_2, \n",
    "                                   filter_height,\n",
    "                                   filter_width_2,\n",
    "                                   output_channels_2,\n",
    "                                   conv_stride=1,\n",
    "                                   pooling_size=10,\n",
    "                                   pooling_stride=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our connected layers. First we flatten the output of the convolutional layers to feed it into the connected network. The number of neurons in the first layer is equal to the size of this flattened array while the second layer size has been defined before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We first flatten the output of the second convolutional layer to feed if in a 2-layer NN\n",
    "shape = conv_layer_2.get_shape().as_list()\n",
    "flatten_layer = tf.reshape(conv_layer_2, shape=[-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "# 1 st fully connected layer\n",
    "num_features = shape[1] * shape[2] * shape[3]\n",
    "hidden_layer_1 = fully_connected_layer(flatten_layer, num_features, num_hidden)\n",
    "\n",
    "# We add dropout to the first layer in order to control overfitting.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "hidden_layer_1 = tf.nn.dropout(hidden_layer_1, keep_prob)\n",
    "\n",
    "# 2nd fully connected layer. relu is set to False since we will apply softmax to this layer when \n",
    "# defining the cost in the next cell\n",
    "hidden_layer_2 = fully_connected_layer(hidden_layer_1, num_hidden, num_labels, relu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_layer_2, labels=y))\n",
    "# optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "# network prediction\n",
    "prediction = tf.argmax(tf.nn.softmax(hidden_layer_2), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "learning rate = 0.0001\n",
      "training_epoch = 100\n",
      "batch_size = 10\n",
      "drop out prob = 0.5\n",
      "-- Conv layer 1 --\n",
      "input_channels_1 = 3\n",
      "filter_width_1 = 30\n",
      "output_channels_1 = 14\n",
      "-- Conv layer 2 --\n",
      "input_channels_2 = 14\n",
      "filter_width_2 = 15\n",
      "output_channels_2 = 36\n",
      "-- Hidden layer --\n",
      "hidden layer size = 1000\n",
      "\n",
      "\n",
      "Epoch n°    0. cost = 1.05199766\n",
      "Epoch n°   10. cost = 0.27474557\n",
      "Epoch n°   20. cost = 0.20813611\n",
      "Epoch n°   30. cost = 0.15137240\n",
      "Epoch n°   40. cost = 0.10558614\n",
      "Epoch n°   50. cost = 0.06818593\n",
      "Epoch n°   60. cost = 0.03583900\n",
      "Epoch n°   70. cost = 0.01746062\n",
      "Epoch n°   80. cost = 0.01177079\n",
      "Epoch n°   90. cost = 0.00840840\n",
      "Training over...\n",
      "\n",
      "\n",
      "Training set report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Walking       1.00      1.00      1.00      1226\n",
      "  Walking upstairs       1.00      1.00      1.00      1073\n",
      "Walking downstairs       1.00      1.00      1.00       986\n",
      "           Sitting       1.00      1.00      1.00      1286\n",
      "          Standing       1.00      1.00      1.00      1374\n",
      "            Laying       1.00      1.00      1.00      1407\n",
      "\n",
      "       avg / total       1.00      1.00      1.00      7352\n",
      "\n",
      "Test set report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Walking       0.97      0.93      0.95       496\n",
      "  Walking upstairs       0.99      0.86      0.92       471\n",
      "Walking downstairs       0.96      0.95      0.96       420\n",
      "           Sitting       0.59      0.63      0.61       491\n",
      "          Standing       0.59      0.76      0.66       532\n",
      "            Laying       0.91      0.74      0.82       537\n",
      "\n",
      "       avg / total       0.83      0.81      0.81      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "print(\"Parameters:\")\n",
    "print(\"learning rate = {}\".format(learning_rate))\n",
    "print(\"training_epoch = {}\".format(training_epoch))\n",
    "print(\"batch_size = {}\".format(batch_size))\n",
    "print(\"drop out prob = {}\".format(prob))\n",
    "print(\"-- Conv layer 1 --\")\n",
    "print(\"input_channels_1 = {}\".format(input_channels_1))\n",
    "print(\"filter_width_1 = {}\".format(filter_width_1))\n",
    "print(\"output_channels_1 = {}\".format(output_channels_1))\n",
    "print(\"-- Conv layer 2 --\")\n",
    "print(\"input_channels_2 = {}\".format(input_channels_2))\n",
    "print(\"filter_width_2 = {}\".format(filter_width_2))\n",
    "print(\"output_channels_2 = {}\".format(output_channels_2))\n",
    "print(\"-- Hidden layer --\")\n",
    "print(\"hidden layer size = {}\".format(num_hidden))\n",
    "print(\"\\n\")\n",
    "\n",
    "cost_log = []\n",
    "\n",
    "for epoch in range(training_epoch):\n",
    "    avg_cost = 0. \n",
    "    num_batch = X_train.shape[0] // batch_size\n",
    "    \n",
    "    # We first shuffle the training data\n",
    "    shuffle = np.random.permutation(X_train.shape[0])\n",
    "    X_train_s = X_train[shuffle, :, :, :]\n",
    "    y_train_s = y_train[shuffle, :]\n",
    "    \n",
    "    for i in range(num_batch):\n",
    "        batch_x = X_train_s[int(i*batch_size):int((i+1)*batch_size), :, :, :]\n",
    "        batch_y = y_train_s[int(i*batch_size):int((i+1)*batch_size), :]\n",
    "        train = sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: prob})\n",
    "        c =  sess.run(cost, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
    "        avg_cost += c/num_batch\n",
    "                \n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch n° {:4d}. cost = {:.8f}\".format(epoch, avg_cost))   \n",
    "        \n",
    "    cost_log.append(avg_cost)    \n",
    "\n",
    "    \n",
    "print(\"Training over...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(range(len(cost_log)), cost_log)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.title(\"Training set cost\")\n",
    "\n",
    "y_train_pred = sess.run(prediction, feed_dict={x: X_train, keep_prob: 1.})\n",
    "print('Training set report')\n",
    "print(classification_report(np.argmax(y_train, axis=1), y_train_pred, target_names=label_names))\n",
    "plt.figure(3)\n",
    "fef.plot_confusion_matrix(confusion_matrix(np.argmax(y_train, axis=1), y_train_pred), label_names)\n",
    "\n",
    "y_test_pred = sess.run(prediction, feed_dict={x: X_test, keep_prob: 1.})\n",
    "print('Test set report')\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_test_pred, target_names=label_names))\n",
    "plt.figure(4)\n",
    "fef.plot_confusion_matrix(confusion_matrix(np.argmax(y_test, axis=1), y_test_pred), label_names)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
