{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  This notebook shows an example of neural networks applied to time series classification using Tensorflow. We build a 2-layer neural network with softmax activation and use the raw time series as input for our neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "import feature_extraction_functions as fef\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn import preprocessing\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set we use is x-axis acceleration of people performing 6 different activities (walking (1), walking upstairs (2), walking downstairs (3), sitting (4), standing (5) laying (6)). More details about the data set [here](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_x_raw shape : (7352, 128)\n",
      "X_test_x_raw shape : (2947, 128)\n",
      "X_train_y_raw shape : (7352, 128)\n",
      "X_test_y_raw shape : (2947, 128)\n",
      "X_train_z_raw shape : (7352, 128)\n",
      "X_test_z_raw shape : (2947, 128)\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../data')\n",
    "\n",
    "\n",
    "# Raw signals\n",
    "# X axis\n",
    "X_train_x_raw = np.loadtxt('X_train.txt')\n",
    "X_test_x_raw = np.loadtxt('X_test.txt')\n",
    "# Y axis\n",
    "X_train_y_raw = np.loadtxt('Xy_train.txt')\n",
    "X_test_y_raw = np.loadtxt('Xy_test.txt')\n",
    "# Z axis\n",
    "X_train_z_raw = np.loadtxt('Xz_train.txt')\n",
    "X_test_z_raw = np.loadtxt('Xz_test.txt')\n",
    "\n",
    "print(\"X_train_x_raw shape : {}\".format(X_train_x_raw.shape))\n",
    "print(\"X_test_x_raw shape : {}\".format(X_test_x_raw.shape))\n",
    "print(\"X_train_y_raw shape : {}\".format(X_train_y_raw.shape))\n",
    "print(\"X_test_y_raw shape : {}\".format(X_test_y_raw.shape))\n",
    "print(\"X_train_z_raw shape : {}\".format(X_train_z_raw.shape))\n",
    "print(\"X_test_z_raw shape : {}\".format(X_test_z_raw.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature extraction helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained at the beginning of this notebook, we are not going to use the raw signals as input for our model but we are going to extract features from the signals. The following functions help us build the feature vectors out of the raw signals we have loaded above.\n",
    "These functions help us extract statistical and geometrical features from raw signals and jerk signals (acceleration first derivative), frequency domain features from raw signals and jerk signals\n",
    "For each sample we extract the following features:\n",
    "  - **x,y and z raw signals** : mean, max, min, standard deviation, skewness, kurtosis, interquartile range, median absolute deviation, area under curve, area under squared curve\n",
    "  - **x,y and z jerk signals (first derivative)** : mean, max, min, standard deviation, skewness, kurtosis, interquartile range, median absolute deviation, area under curve, area under squared curve\n",
    "  - **x,y and z raw signals Discrete Fourrier Transform**: mean, max, min, standard deviation, skewness, kurtosis, interquartile range, median absolute deviation, area under curve, area under squared curve, weighted mean frequency, 5 first DFT coefficients, 5 first local maxima of DFT coefficients and their corresponding frequencies.\n",
    "  - **x,y and z jerk signals Discrete Fourrier Transform**: mean, max, min, standard deviation, skewness, kurtosis, interquartile range, median absolute deviation, area under curve, area under squared curve, weighted mean frequency, 5 first DFT coefficients, 5 first local maxima of DFT coefficients and their corresponding frequencies.\n",
    "  - **x,y and z correlation coefficients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "from scipy.fftpack import fft, fftfreq \n",
    "from scipy.signal import argrelextrema\n",
    "import operator\n",
    "\n",
    "def stat_area_features(x, Te=1.0):\n",
    "\n",
    "    mean_ts = np.mean(x, axis=1).reshape(-1,1) # mean\n",
    "    max_ts = np.amax(x, axis=1).reshape(-1,1) # max\n",
    "    min_ts = np.amin(x, axis=1).reshape(-1,1) # min\n",
    "    std_ts = np.std(x, axis=1).reshape(-1,1) # std\n",
    "    skew_ts = st.skew(x, axis=1).reshape(-1,1) # skew\n",
    "    kurtosis_ts = st.kurtosis(x, axis=1).reshape(-1,1) # kurtosis \n",
    "    iqr_ts = st.iqr(x, axis=1).reshape(-1,1) # interquartile rante\n",
    "    mad_ts = np.median(np.sort(abs(x - np.median(x, axis=1).reshape(-1,1)),\n",
    "                               axis=1), axis=1).reshape(-1,1) # median absolute deviation\n",
    "    area_ts = np.trapz(x, axis=1, dx=Te).reshape(-1,1) # area under curve\n",
    "    sq_area_ts = np.trapz(x ** 2, axis=1, dx=Te).reshape(-1,1) # area under curve ** 2\n",
    "\n",
    "    return np.concatenate((mean_ts,max_ts,min_ts,std_ts,skew_ts,kurtosis_ts,\n",
    "                           iqr_ts,mad_ts,area_ts,sq_area_ts), axis=1)\n",
    "\n",
    "def frequency_domain_features(x, Te=1.0):\n",
    "\n",
    "    # As the DFT coefficients and their corresponding frequencies are symetrical arrays\n",
    "    # with respect to the middle of the array we need to know if the number of readings \n",
    "    # in x is even or odd to then split the arrays...\n",
    "    if x.shape[1]%2 == 0:\n",
    "        N = int(x.shape[1]/2)\n",
    "    else:\n",
    "        N = int(x.shape[1]/2) - 1\n",
    "    xf = np.repeat(fftfreq(x.shape[1],d=Te)[:N].reshape(1,-1), x.shape[0], axis=0) # frequencies\n",
    "    dft = np.abs(fft(x, axis=1))[:,:N] # DFT coefficients   \n",
    "    \n",
    "    # statistical and area features\n",
    "    dft_features = stat_area_features(dft, Te=1.0)\n",
    "    # weighted mean frequency\n",
    "    dft_weighted_mean_f = np.average(xf, axis=1, weights=dft).reshape(-1,1)\n",
    "    # 5 first DFT coefficients \n",
    "    dft_first_coef = dft[:,:5]    \n",
    "    # 5 first local maxima of DFT coefficients and their corresponding frequencies\n",
    "    dft_max_coef = np.zeros((x.shape[0],5))\n",
    "    dft_max_coef_f = np.zeros((x.shape[0],5))\n",
    "    for row in range(x.shape[0]):\n",
    "        # finds all local maximas indexes\n",
    "        extrema_ind = argrelextrema(dft[row,:], np.greater, axis=0) \n",
    "        # makes a list of tuples (DFT_i, f_i) of all the local maxima\n",
    "        # and keeps the 5 biggest...\n",
    "        extrema_row = sorted([(dft[row,:][j],xf[row,j]) for j in extrema_ind[0]],\n",
    "                             key=operator.itemgetter(0), reverse=True)[:5] \n",
    "        for i, ext in enumerate(extrema_row):\n",
    "            dft_max_coef[row,i] = ext[0]\n",
    "            dft_max_coef_f[row,i] = ext[1]    \n",
    "    \n",
    "    return np.concatenate((dft_features,dft_weighted_mean_f,dft_first_coef,\n",
    "                           dft_max_coef,dft_max_coef_f), axis=1)\n",
    "\n",
    "def make_feature_vector(x,y,z, Te=1.0):\n",
    "\n",
    "    # Raw signals :  stat and area features\n",
    "    features_xt = stat_area_features(x, Te=Te)\n",
    "    features_yt = stat_area_features(y, Te=Te)\n",
    "    features_zt = stat_area_features(z, Te=Te)\n",
    "    \n",
    "    # Jerk signals :  stat and area features\n",
    "    features_xt_jerk = stat_area_features((x[:,1:]-x[:,:-1])/Te, Te=Te)\n",
    "    features_yt_jerk = stat_area_features((y[:,1:]-y[:,:-1])/Te, Te=Te)\n",
    "    features_zt_jerk = stat_area_features((z[:,1:]-z[:,:-1])/Te, Te=Te) \n",
    "    \n",
    "    # Raw signals : frequency domain features \n",
    "    features_xf = frequency_domain_features(x, Te=1/Te)\n",
    "    features_yf = frequency_domain_features(y, Te=1/Te)\n",
    "    features_zf = frequency_domain_features(z, Te=1/Te)\n",
    "    \n",
    "    # Jerk signals : frequency domain features \n",
    "    features_xf_jerk = frequency_domain_features((x[:,1:]-x[:,:-1])/Te, Te=1/Te)\n",
    "    features_yf_jerk = frequency_domain_features((y[:,1:]-y[:,:-1])/Te, Te=1/Te)\n",
    "    features_zf_jerk = frequency_domain_features((z[:,1:]-z[:,:-1])/Te, Te=1/Te)\n",
    "    \n",
    "    # Raw signals correlation coefficient between axis\n",
    "    cor = np.empty((x.shape[0],3))\n",
    "    for row in range(x.shape[0]):\n",
    "        xyz_matrix = np.concatenate((x[row,:].reshape(1,-1),y[row,:].reshape(1,-1),\n",
    "                                     z[row,:].reshape(1,-1)), axis=0)\n",
    "        cor[row,0] = np.corrcoef(xyz_matrix)[0,1]\n",
    "        cor[row,1] = np.corrcoef(xyz_matrix)[0,2]\n",
    "        cor[row,2] = np.corrcoef(xyz_matrix)[1,2]\n",
    "    \n",
    "    return np.concatenate((features_xt, features_yt, features_zt,\n",
    "                           features_xt_jerk, features_yt_jerk, features_zt_jerk,\n",
    "                           features_xf, features_yf, features_zf,\n",
    "                           features_xf_jerk, features_yf_jerk, features_zf_jerk,\n",
    "                           cor), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape : (7352, 219)\n",
      "X_test shape: (2947, 219)\n"
     ]
    }
   ],
   "source": [
    "X_train = make_feature_vector(X_train_x_raw, X_train_y_raw, X_train_z_raw, Te=1/50)\n",
    "X_test = make_feature_vector(X_test_x_raw, X_test_y_raw, X_test_z_raw, Te=1/50)\n",
    "\n",
    "print(\"X_train shape : {}\".format(X_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale the features (standard scaler i.e. each feature column has a zero mean and one standard deviation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train) \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape : (7352, 6)\n",
      "y_test shape : (2947, 6)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.loadtxt('y_train.txt') - 1\n",
    "y_train = (np.arange(np.unique(y_train).shape[0]) == y_train[:, None]).astype(int)\n",
    "\n",
    "y_test = np.loadtxt('y_test.txt') - 1\n",
    "y_test = (np.arange(np.unique(y_test).shape[0]) == y_test[:, None]).astype(int)\n",
    "\n",
    "print(\"y_train shape : {}\".format(y_train.shape))\n",
    "print(\"y_test shape : {}\".format(y_test.shape))\n",
    "\n",
    "label_names = ['Walking', 'Walking upstairs', 'Walking downstairs', 'Sitting', 'Standing', 'Laying']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Storing number of features and number of labels for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = X_train.shape[1]\n",
    "n_labels = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epoch = 1500\n",
    "batch_size = 100\n",
    "display_step = 100\n",
    "num_layer_1 = n_features\n",
    "num_layer_2 = (n_features + n_labels) // 4\n",
    "l = 1.0e-4\n",
    "prob = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Tensorflow graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Neural Network input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, n_features])\n",
    "y = tf.placeholder(tf.float32, shape=[None, n_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here we build the actual network with two layers: the first layer a n_features number of neurons with softmax activation and a second layer with num_layer_2 neurons and no activation function since we are going to apply softmax to the output of this second layer when defining the neural network cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1st hidden layer\n",
    "W_1 = tf.Variable(tf.random_normal([num_layer_1, num_layer_2], stddev=0.01))\n",
    "b_1 = tf.Variable(tf.zeros([num_layer_2]))\n",
    "y_1 = tf.nn.softmax(tf.matmul(x, W_1) + b_1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "y_1 = tf.nn.dropout(y_1, keep_prob)\n",
    "\n",
    "# Output layer\n",
    "W_2 = tf.Variable(tf.random_normal([num_layer_2, n_labels], stddev=0.01))\n",
    "b_2 = tf.Variable(tf.zeros([n_labels]))\n",
    "y_pred = tf.matmul(y_1, W_2) + b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Cost and optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lam = tf.placeholder(tf.float32)\n",
    "\n",
    "# Cost function with L2 regularization\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_pred) + \n",
    "                      lam * (tf.nn.l2_loss(W_1) + tf.nn.l2_loss(W_2)))\n",
    "# optimizer\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "# prediction\n",
    "prediction = tf.nn.softmax(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramters:\n",
      "learning rate = 0.001\n",
      "training_epoch = 1500\n",
      "batch_size = 100\n",
      "-- Network structure -- \n",
      "num_layer_1 = 219\n",
      "num_layer_2 = 56\n",
      "-- Regularization --\n",
      "drop out prob = 1.0\n",
      "L2 regularization = 0.0001\n",
      "\n",
      "\n",
      "Epoch n°    0. cost = 1.74471654.\n",
      "Epoch n°  100. cost = 0.42173810.\n",
      "Epoch n°  200. cost = 0.31790106.\n",
      "Epoch n°  300. cost = 0.29808484.\n",
      "Epoch n°  400. cost = 0.29489058.\n",
      "Epoch n°  500. cost = 0.29288995.\n",
      "Epoch n°  600. cost = 0.29001779.\n",
      "Epoch n°  700. cost = 0.28662263.\n",
      "Epoch n°  800. cost = 0.28638686.\n",
      "Epoch n°  900. cost = 0.28356351.\n",
      "Epoch n° 1000. cost = 0.27609362.\n",
      "Epoch n° 1100. cost = 0.27500384.\n",
      "Epoch n° 1200. cost = 0.27400966.\n",
      "Epoch n° 1300. cost = 0.27102072.\n",
      "Epoch n° 1400. cost = 0.27230263.\n",
      "Training over...\n",
      "\n",
      "\n",
      "Training set report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Walking       1.00      1.00      1.00      1226\n",
      "  Walking upstairs       1.00      1.00      1.00      1073\n",
      "Walking downstairs       1.00      1.00      1.00       986\n",
      "           Sitting       0.72      0.66      0.69      1286\n",
      "          Standing       0.76      0.84      0.80      1374\n",
      "            Laying       0.90      0.89      0.89      1407\n",
      "\n",
      "       avg / total       0.89      0.89      0.89      7352\n",
      "\n",
      "Test set report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Walking       0.89      0.98      0.93       496\n",
      "  Walking upstairs       0.97      0.89      0.92       471\n",
      "Walking downstairs       0.99      0.95      0.97       420\n",
      "           Sitting       0.67      0.60      0.63       491\n",
      "          Standing       0.69      0.80      0.75       532\n",
      "            Laying       0.90      0.84      0.87       537\n",
      "\n",
      "       avg / total       0.84      0.84      0.84      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "cost_log = []\n",
    "\n",
    "print(\"Paramters:\")\n",
    "print(\"learning rate = {}\".format(learning_rate))\n",
    "print(\"training_epoch = {}\".format(training_epoch))\n",
    "print(\"batch_size = {}\".format(batch_size))\n",
    "print(\"-- Network structure -- \")\n",
    "print(\"num_layer_1 = {}\".format(num_layer_1))\n",
    "print(\"num_layer_2 = {}\".format(num_layer_2))\n",
    "print(\"-- Regularization --\")\n",
    "print(\"drop out prob = {}\".format(prob))\n",
    "print(\"L2 regularization = {}\".format(l))\n",
    "print(\"\\n\")\n",
    "\n",
    "for epoch in range(training_epoch):\n",
    "    avg_cost = 0.\n",
    "    num_batch = X_train.shape[0] // batch_size\n",
    "    \n",
    "    # We first shuffle the training data\n",
    "    shuffle = np.random.permutation(X_train.shape[0])\n",
    "    X_train_s = X_train[shuffle, :]\n",
    "    y_train_s = y_train[shuffle, :]\n",
    "    \n",
    "    for i in range(num_batch):\n",
    "        batch_x = X_train_s[int(i*batch_size):int((i+1)*batch_size), :]\n",
    "        batch_y = y_train_s[int(i*batch_size):int((i+1)*batch_size), :]\n",
    "        train = sess.run(train_step, feed_dict={x: batch_x, y: batch_y, keep_prob: prob, lam: l})\n",
    "        c =  sess.run(cost, feed_dict={x: batch_x, y: batch_y, keep_prob: 1., lam: 0.})        \n",
    "        avg_cost += c/num_batch\n",
    "        \n",
    "    # Train and cv cost log\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch n° {:4d}. cost = {:.8f}.\".format(epoch, avg_cost))\n",
    "        \n",
    "    cost_log.append(avg_cost)\n",
    "    \n",
    "\n",
    "print(\"Training over...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(range(len(cost_log)), cost_log)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.title(\"Training set cost\")\n",
    "\n",
    "\n",
    "y_train_pred = sess.run(tf.argmax(prediction,1), feed_dict={x: X_train, keep_prob: 1.})\n",
    "print('Training set report')\n",
    "print(classification_report(np.argmax(y_train, axis=1), y_train_pred, target_names=label_names))\n",
    "plt.figure(3)\n",
    "fef.plot_confusion_matrix(confusion_matrix(np.argmax(y_train, axis=1), y_train_pred), label_names)\n",
    "\n",
    "y_test_pred = sess.run(tf.argmax(prediction,1), feed_dict={x: X_test, keep_prob: 1.})\n",
    "print('Test set report')\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_test_pred, target_names=label_names))\n",
    "plt.figure(4)\n",
    "fef.plot_confusion_matrix(confusion_matrix(np.argmax(y_test, axis=1), y_test_pred), label_names)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
