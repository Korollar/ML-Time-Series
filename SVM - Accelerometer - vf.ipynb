{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This notebook shows an example of Support Vector Machine (SVM) applied to time series classification (accelerometer data). The task here is to classify 6 different types of activities (walking, walking upstairs, walking downstairs, sitting, standing, laying) based on x, y and z accelerometer signals. In this notebook we are going to extract features from raw signals and use this as input for the SVM. We optimize hyperparameters C and gamma to achieve the best f1-score on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the raw accelerometer signals of the 3 available axis (x, y and z). Each sample of the data set is a 2.56s window of an activity being performed recorded at a 50Hz rate which makes 128 readings per sample per axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_x_raw shape : (7352, 128)\n",
      "X_test_x_raw shape : (2947, 128)\n",
      "X_train_y_raw shape : (7352, 128)\n",
      "X_test_y_raw shape : (2947, 128)\n",
      "X_train_z_raw shape : (7352, 128)\n",
      "X_test_z_raw shape : (2947, 128)\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../data')\n",
    "# Raw signals\n",
    "# X axis\n",
    "X_train_x_raw = np.loadtxt('X_train.txt')\n",
    "X_test_x_raw = np.loadtxt('X_test.txt')\n",
    "# Y axis\n",
    "X_train_y_raw = np.loadtxt('Xy_train.txt')\n",
    "X_test_y_raw = np.loadtxt('Xy_test.txt')\n",
    "# Z axis\n",
    "X_train_z_raw = np.loadtxt('Xz_train.txt')\n",
    "X_test_z_raw = np.loadtxt('Xz_test.txt')\n",
    "\n",
    "print(\"X_train_x_raw shape : {}\".format(X_train_x_raw.shape))\n",
    "print(\"X_test_x_raw shape : {}\".format(X_test_x_raw.shape))\n",
    "print(\"X_train_y_raw shape : {}\".format(X_train_y_raw.shape))\n",
    "print(\"X_test_y_raw shape : {}\".format(X_test_y_raw.shape))\n",
    "print(\"X_train_z_raw shape : {}\".format(X_train_z_raw.shape))\n",
    "print(\"X_test_z_raw shape : {}\".format(X_test_z_raw.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We load the label vectors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352,)\n",
      "(2947,)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.loadtxt('y_train.txt')\n",
    "y_test = np.loadtxt('y_test.txt')\n",
    "\n",
    "label_names = ['Walking', 'Walking upstairs', 'Walking downstairs', 'Sitting', 'Standing', 'Laying']\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained at the beginning of this notebook, we are not going to use the raw signals as input for our model but we are going to extract features from the signals. The following functions help us build the feature vectors out of the raw signals we have loaded above.\n",
    "These functions help us extract statistical and geometrical features from raw signals and jerk signals (acceleration first derivative), frequency domain features from raw signals and jerk signals\n",
    "For each sample we extract the following features:\n",
    "  - **x,y and z raw signals** : mean, max, min, standard deviation, skewness, kurtosis, interquartile range, median absolute deviation, area under curve, area under squared curve\n",
    "  - **x,y and z jerk signals (first derivative)** : mean, max, min, standard deviation, skewness, kurtosis, interquartile range, median absolute deviation, area under curve, area under squared curve\n",
    "  - **x,y and z raw signals Discrete Fourrier Transform**: mean, max, min, standard deviation, skewness, kurtosis, interquartile range, median absolute deviation, area under curve, area under squared curve, weighted mean frequency, 5 first DFT coefficients, 5 first local maxima of DFT coefficients and their corresponding frequencies.\n",
    "  - **x,y and z jerk signals Discrete Fourrier Transform**: mean, max, min, standard deviation, skewness, kurtosis, interquartile range, median absolute deviation, area under curve, area under squared curve, weighted mean frequency, 5 first DFT coefficients, 5 first local maxima of DFT coefficients and their corresponding frequencies.\n",
    "  - **x,y and z correlation coefficients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "from scipy.fftpack import fft, fftfreq \n",
    "from scipy.signal import argrelextrema\n",
    "import operator\n",
    "\n",
    "def stat_area_features(x, Te=1.0):\n",
    "\n",
    "    mean_ts = np.mean(x, axis=1).reshape(-1,1) # mean\n",
    "    max_ts = np.amax(x, axis=1).reshape(-1,1) # max\n",
    "    min_ts = np.amin(x, axis=1).reshape(-1,1) # min\n",
    "    std_ts = np.std(x, axis=1).reshape(-1,1) # std\n",
    "    skew_ts = st.skew(x, axis=1).reshape(-1,1) # skew\n",
    "    kurtosis_ts = st.kurtosis(x, axis=1).reshape(-1,1) # kurtosis \n",
    "    iqr_ts = st.iqr(x, axis=1).reshape(-1,1) # interquartile rante\n",
    "    mad_ts = np.median(np.sort(abs(x - np.median(x, axis=1).reshape(-1,1)),\n",
    "                               axis=1), axis=1).reshape(-1,1) # median absolute deviation\n",
    "    area_ts = np.trapz(x, axis=1, dx=Te).reshape(-1,1) # area under curve\n",
    "    sq_area_ts = np.trapz(x ** 2, axis=1, dx=Te).reshape(-1,1) # area under curve ** 2\n",
    "\n",
    "    return np.concatenate((mean_ts,max_ts,min_ts,std_ts,skew_ts,kurtosis_ts,\n",
    "                           iqr_ts,mad_ts,area_ts,sq_area_ts), axis=1)\n",
    "\n",
    "def frequency_domain_features(x, Te=1.0):\n",
    "\n",
    "    # As the DFT coefficients and their corresponding frequencies are symetrical arrays\n",
    "    # with respect to the middle of the array we need to know if the number of readings \n",
    "    # in x is even or odd to then split the arrays...\n",
    "    if x.shape[1]%2 == 0:\n",
    "        N = int(x.shape[1]/2)\n",
    "    else:\n",
    "        N = int(x.shape[1]/2) - 1\n",
    "    xf = np.repeat(fftfreq(x.shape[1],d=Te)[:N].reshape(1,-1), x.shape[0], axis=0) # frequencies\n",
    "    dft = np.abs(fft(x, axis=1))[:,:N] # DFT coefficients   \n",
    "    \n",
    "    # statistical and area features\n",
    "    dft_features = stat_area_features(dft, Te=1.0)\n",
    "    # weighted mean frequency\n",
    "    dft_weighted_mean_f = np.average(xf, axis=1, weights=dft).reshape(-1,1)\n",
    "    # 5 first DFT coefficients \n",
    "    dft_first_coef = dft[:,:5]    \n",
    "    # 5 first local maxima of DFT coefficients and their corresponding frequencies\n",
    "    dft_max_coef = np.zeros((x.shape[0],5))\n",
    "    dft_max_coef_f = np.zeros((x.shape[0],5))\n",
    "    for row in range(x.shape[0]):\n",
    "        # finds all local maximas indexes\n",
    "        extrema_ind = argrelextrema(dft[row,:], np.greater, axis=0) \n",
    "        # makes a list of tuples (DFT_i, f_i) of all the local maxima\n",
    "        # and keeps the 5 biggest...\n",
    "        extrema_row = sorted([(dft[row,:][j],xf[row,j]) for j in extrema_ind[0]],\n",
    "                             key=operator.itemgetter(0), reverse=True)[:5] \n",
    "        for i, ext in enumerate(extrema_row):\n",
    "            dft_max_coef[row,i] = ext[0]\n",
    "            dft_max_coef_f[row,i] = ext[1]    \n",
    "    \n",
    "    return np.concatenate((dft_features,dft_weighted_mean_f,dft_first_coef,\n",
    "                           dft_max_coef,dft_max_coef_f), axis=1)\n",
    "\n",
    "def make_feature_vector(x,y,z, Te=1.0):\n",
    "\n",
    "    # Raw signals :  stat and area features\n",
    "    features_xt = stat_area_features(x, Te=Te)\n",
    "    features_yt = stat_area_features(y, Te=Te)\n",
    "    features_zt = stat_area_features(z, Te=Te)\n",
    "    \n",
    "    # Jerk signals :  stat and area features\n",
    "    features_xt_jerk = stat_area_features((x[:,1:]-x[:,:-1])/Te, Te=Te)\n",
    "    features_yt_jerk = stat_area_features((y[:,1:]-y[:,:-1])/Te, Te=Te)\n",
    "    features_zt_jerk = stat_area_features((z[:,1:]-z[:,:-1])/Te, Te=Te) \n",
    "    \n",
    "    # Raw signals : frequency domain features \n",
    "    features_xf = frequency_domain_features(x, Te=1/Te)\n",
    "    features_yf = frequency_domain_features(y, Te=1/Te)\n",
    "    features_zf = frequency_domain_features(z, Te=1/Te)\n",
    "    \n",
    "    # Jerk signals : frequency domain features \n",
    "    features_xf_jerk = frequency_domain_features((x[:,1:]-x[:,:-1])/Te, Te=1/Te)\n",
    "    features_yf_jerk = frequency_domain_features((y[:,1:]-y[:,:-1])/Te, Te=1/Te)\n",
    "    features_zf_jerk = frequency_domain_features((z[:,1:]-z[:,:-1])/Te, Te=1/Te)\n",
    "    \n",
    "    # Raw signals correlation coefficient between axis\n",
    "    cor = np.empty((x.shape[0],3))\n",
    "    for row in range(x.shape[0]):\n",
    "        xyz_matrix = np.concatenate((x[row,:].reshape(1,-1),y[row,:].reshape(1,-1),\n",
    "                                     z[row,:].reshape(1,-1)), axis=0)\n",
    "        cor[row,0] = np.corrcoef(xyz_matrix)[0,1]\n",
    "        cor[row,1] = np.corrcoef(xyz_matrix)[0,2]\n",
    "        cor[row,2] = np.corrcoef(xyz_matrix)[1,2]\n",
    "    \n",
    "    return np.concatenate((features_xt, features_yt, features_zt,\n",
    "                           features_xt_jerk, features_yt_jerk, features_zt_jerk,\n",
    "                           features_xf, features_yf, features_zf,\n",
    "                           features_xf_jerk, features_yf_jerk, features_zf_jerk,\n",
    "                           cor), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape : (7352, 219)\n",
      "X_test shape: (2947, 219)\n"
     ]
    }
   ],
   "source": [
    "X_train = make_feature_vector(X_train_x_raw, X_train_y_raw, X_train_z_raw, Te=1/50)\n",
    "X_test = make_feature_vector(X_test_x_raw, X_test_y_raw, X_test_z_raw, Te=1/50)\n",
    "\n",
    "print(\"X_train shape : {}\".format(X_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale the features (standard scaler i.e. each feature column has a zero mean and one standard deviation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train) \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the SVM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to implement a SVM model with a RBF kernel. First we are going to build a grid search function to optimize the RBF kernel hyperparameters C and gamma to achieve the best f1 score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperparameters_opt_RBF(X_train, y_train, X_test, y_test,\n",
    "                            param_range_C, param_range_gamma):\n",
    "\n",
    "    accuracy_matrix = np.zeros((param_range_C.shape[0],param_range_gamma.shape[0]))\n",
    "    for i,C in enumerate(param_range_C):\n",
    "        for j,gamma in enumerate(param_range_gamma):\n",
    "            clf_r_l = svm.SVC(kernel='rbf', C=C, gamma=gamma)\n",
    "            clf_r_l.fit(X_train, y_train)\n",
    "            accuracy_matrix[i,j] = f1_score(y_test ,clf_r_l.predict(X_test), average='macro')\n",
    "    C_ind, gamma_ind = np.unravel_index(np.argmax(accuracy_matrix), accuracy_matrix.shape)\n",
    "\n",
    "    return param_range_C[C_ind], param_range_gamma[gamma_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters optimization results:\n",
      "C: 1.0\n",
      "gamma: 0.01\n",
      "\n",
      "\n",
      "Training set report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Walking       1.00      1.00      1.00      1226\n",
      "  Walking upstairs       1.00      1.00      1.00      1073\n",
      "Walking downstairs       1.00      1.00      1.00       986\n",
      "           Sitting       0.83      0.73      0.78      1286\n",
      "          Standing       0.81      0.88      0.84      1374\n",
      "            Laying       0.92      0.94      0.93      1407\n",
      "\n",
      "       avg / total       0.92      0.92      0.92      7352\n",
      "\n",
      "Test set report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Walking       0.94      0.98      0.96       496\n",
      "  Walking upstairs       0.94      0.95      0.94       471\n",
      "Walking downstairs       0.97      0.91      0.94       420\n",
      "           Sitting       0.66      0.53      0.59       491\n",
      "          Standing       0.68      0.83      0.75       532\n",
      "            Laying       0.87      0.84      0.85       537\n",
      "\n",
      "       avg / total       0.84      0.84      0.83      2947\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fef' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1c1b625d88c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mfef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mfef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fef' is not defined"
     ]
    }
   ],
   "source": [
    "# C = 1\n",
    "# gamma = 0.01\n",
    "# C and gamma test range\n",
    "param_range = np.array([0.01, 0.1, 1, 10, 100, 1000])\n",
    "# Optimizing C and gamma parameters to achieve the best f1 score \n",
    "C, gamma = hyperparameters_opt_RBF(X_train, y_train, X_test, y_test, param_range, param_range)\n",
    "\n",
    "print(\"Hyperparameters optimization results:\")\n",
    "print(\"C: {}\".format(C))\n",
    "print(\"gamma: {}\".format(gamma))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Training the model with the optimized hyperparameters\n",
    "clf_r_l = svm.SVC(kernel='rbf', C=C, gamma=gamma)\n",
    "clf_r_l.fit(X_train, y_train)\n",
    "y_pred_tr = clf_r_l.predict(X_train)\n",
    "y_pred_te = clf_r_l.predict(X_test)\n",
    "\n",
    "print('Training set report')\n",
    "print(classification_report(y_train, y_pred_tr, target_names=label_names))\n",
    "print('Test set report')\n",
    "print(classification_report(y_test, y_pred_te, target_names=label_names))\n",
    "\n",
    "plt.figure(1)\n",
    "fef.plot_confusion_matrix(confusion_matrix(y_train, y_pred_tr), label_names)\n",
    "plt.figure(2)\n",
    "fef.plot_confusion_matrix(confusion_matrix(y_test, y_pred_te), label_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hyperparameters optimization results:\n",
    "C: 1.0\n",
    "gamma: 0.01\n",
    "\n",
    "\n",
    "Training set report\n",
    "                    precision    recall  f1-score   support\n",
    "\n",
    "           Walking       1.00      1.00      1.00      1226\n",
    "  Walking upstairs       1.00      1.00      1.00      1073\n",
    "Walking downstairs       1.00      1.00      1.00       986\n",
    "           Sitting       0.83      0.73      0.78      1286\n",
    "          Standing       0.81      0.88      0.84      1374\n",
    "            Laying       0.92      0.94      0.93      1407\n",
    "\n",
    "       avg / total       0.92      0.92      0.92      7352\n",
    "\n",
    "Test set report\n",
    "                    precision    recall  f1-score   support\n",
    "\n",
    "           Walking       0.94      0.98      0.96       496\n",
    "  Walking upstairs       0.94      0.95      0.94       471\n",
    "Walking downstairs       0.97      0.91      0.94       420\n",
    "           Sitting       0.66      0.53      0.59       491\n",
    "          Standing       0.68      0.83      0.75       532\n",
    "            Laying       0.87      0.84      0.85       537\n",
    "\n",
    "       avg / total       0.84      0.84      0.83      2947"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
